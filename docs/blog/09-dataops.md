Claro! Aqui está a tradução:

---

Por volta de 2007, surgiu um framework chamado DevOps no desenvolvimento de software, com o objetivo de romper os silos entre as equipes de desenvolvimento de software, que escrevem e testam código, e as equipes de implantação de software, que implantam e mantêm o código. O DevOps se inspira em outras metodologias bem conhecidas, incluindo lean e ágil, para eliminar gargalos, reduzir desperdícios, identificar rapidamente problemas e realizar iterações rápidas. O movimento DevOps resultou em ciclos de lançamento mais rápidos e em uma qualidade aprimorada para os produtos de software. À medida que o campo de dados amadureceu, adotamos uma abordagem similar, conhecida como DataOps, para o desenvolvimento de produtos de dados. Assim como o DevOps melhora o processo de desenvolvimento e a qualidade dos produtos de software, o DataOps visa aprimorar o processo de desenvolvimento e a qualidade dos produtos de dados. 

O DataOps é, antes de tudo, um conjunto de hábitos e práticas culturais que você pode adotar. Isso inclui priorizar a comunicação e colaboração com outros stakeholders do negócio, aprender continuamente com seus sucessos e falhas, e adotar uma abordagem de iteração rápida para trabalhar em melhorias nos seus sistemas e processos. Esses também são os hábitos e práticas culturais do DevOps, e foram diretamente inspirados na metodologia ágil, que é um framework de gerenciamento de projetos focado na entrega de trabalho em etapas incrementais e iterativas.

Em termos dos elementos técnicos do DataOps, existem três pilares principais. O primeiro pilar, à esquerda, é a automação. O segundo pilar é a observabilidade e monitoramento. E, finalmente, o último pilar é a resposta a incidentes. Esses pilares são semelhantes aos componentes centrais do DevOps, onde o objetivo final é fornecer funcionalidades e recursos específicos em um produto de software. Mas, no DataOps, o objetivo é fornecer produtos de dados de alta qualidade, onde você pode pensar em um produto de dados como qualquer dado ou sistema de dados que você está fornecendo aos usuários finais. 

Vamos examinar mais de perto cada um desses três pilares do DataOps. Em relação à automação, uma das práticas do DevOps que acelerou o ciclo de vida da construção de software é conhecida como integração contínua e entrega contínua, ou CI/CD, para abreviar. Com o CI/CD, os desenvolvedores conseguem automatizar muitos dos processos manuais necessários para construir, testar e implantar código. Essa automação resulta não apenas em ciclos de revisão e implantação mais rápidos, mas também em menos erros, tornando as equipes de software mais eficientes e eficazes na construção de produtos de software de alta qualidade. O DataOps emprega uma estrutura de automação similar ao processamento de dados, assim como o DevOps se aplica ao desenvolvimento de software. Dentro do DataOps, o objetivo de alto nível da gestão de mudanças automatizada permanece o mesmo, por exemplo, quando se trata de gerenciar mudanças no código, na configuração ou no ambiente. Além disso, o DataOps está focado na gestão de mudanças em pipelines de processamento de dados e nos próprios dados.

Para entender como a automação se aplica ao processamento de dados, vamos imaginar que você acabou de começar em uma pequena organização e foi designado para construir um pipeline de dados que começa com a ingestão de dados de vários sistemas de origem. Então, talvez você esteja ingerindo dados de um banco de dados, além de alguns arquivos e uma API ou uma plataforma de compartilhamento de dados. Depois, talvez você esteja realizando algumas transformações durante o processo de ingestão e armazenando os dados ingeridos em um sistema de armazenamento, talvez um banco de dados. E digamos que você tenha dois casos de uso finais que está atendendo, um para análise e outro para aprendizado de máquina. Em seguida, vamos supor que você esteja realizando mais transformações, talvez modelando e agregando os dados antes de enviá-los para outro sistema de armazenamento e torná-los disponíveis para os usuários finais. Assim, você terá essencialmente dois pipelines para as etapas de transformação e atendimento.

Se você for o primeiro engenheiro de dados dessa organização e estiver nos estágios iniciais de desenvolvimento dos seus sistemas de dados, pode optar por executar manualmente as várias tarefas nesse pipeline de dados, como iniciar manualmente cada um desses processos de ingestão. Depois que esses processos forem concluídos, você executaria manualmente cada uma das etapas subsequentes nas fases de transformação, armazenamento e atendimento. Essa poderia ser uma abordagem razoável para começar rapidamente e prototipar alguns aspectos do seu pipeline de dados a longo prazo. No entanto, esse tipo de execução manual em várias etapas estará sujeito a erros e será ineficiente, pois requer que você execute manualmente cada tarefa com um nível mínimo de automação.

Você pode optar por uma abordagem de agendamento puro, ou seja, definir que cada tarefa em seu pipeline comece em um determinado horário do dia. Então, talvez você inicie todas essas tarefas de ingestão à meia-noite todas as noites. Em seguida, você estimaria quanto tempo leva para todos os dados serem ingeridos e carregados em seu sistema de armazenamento. Depois, você poderia agendar as tarefas de transformação subsequentes para começar após isso, e assim por diante, até todas as tarefas em seu pipeline. Isso é chamado de agendamento porque você cria um cronograma para iniciar automaticamente cada uma das tarefas em seu pipeline de dados.

Para levar as coisas ao próximo nível de automação do DataOps, você poderia adotar um framework de orquestração como o Airflow. Os frameworks de orquestração verificam as dependências entre as tarefas dentro do seu pipeline de dados antes de cada tarefa ser executada. Assim, você pode decidir o horário e a frequência com que deseja que a primeira tarefa do seu pipeline comece. Em seguida, o framework de orquestração iniciará automaticamente as tarefas subsequentes uma vez que as anteriores tenham sido concluídas com sucesso. O framework de orquestração também pode notificá-lo quando houver um erro em qualquer uma das tarefas, de modo que as tarefas subsequentes, que dependem das anteriores, não comecem quando não deveriam. Muitos frameworks de orquestração não apenas automatizam a execução de tarefas em seus pipelines de dados, mas também aprimoram o desenvolvimento desses pipelines, permitindo a verificação e implantação automáticas de novos aspectos do seu pipeline de dados, semelhante ao processo de CI/CD para a implantação de software.

Quando se trata do próximo pilar, que é a observabilidade e monitoramento, a principal coisa que você precisa ter em mente é que qualquer pipeline de dados que você configurar está destinado a falhar eventualmente. Para citar Werner Vogels, o CTO da Amazon Web Services, "tudo falha o tempo todo". Isso significa que, se você não estiver observando e monitorando de perto seus sistemas de dados, ficará pego de surpresa quando eles falharem. No pior cenário, você pode só se dar conta dessas falhas do sistema quando os stakeholders a jusante descobrirem esses problemas por conta própria, por exemplo, em seus relatórios ou painéis de análise. Em meu próprio trabalho com clientes, vi inúmeros casos de dados ruins persistindo em relatórios por meses ou até anos devido a falhas não descobertas em sistemas de processamento de dados. Esses tipos de falhas podem resultar em desperdício de tempo e dinheiro, levar a decisões mal informadas e, em última análise, podem custar seu emprego se os stakeholders perderem a confiança em seu trabalho. Portanto, a observabilidade e o monitoramento são aspectos cruciais dos sistemas de dados que você constrói.

O terceiro pilar do DataOps é a resposta a incidentes, que se refere ao uso das capacidades de observabilidade e monitoramento que você configurou para identificar rapidamente as causas raízes de um incidente e resolvê-lo da maneira mais rápida e confiável possível. Como já disse, as coisas vão quebrar e é apenas uma questão de tempo até que isso aconteça. Com a resposta a incidentes, não se trata apenas da tecnologia e das ferramentas que você usa para identificar e responder a um problema. Também se trata de comunicação aberta e sem culpa, bem como da coordenação dos esforços dos membros da equipe de dados que estão respondendo ao incidente, além de outros membros da organização. Como engenheiro de dados, você deve estar proativamente encontrando problemas antes que eles sejam reportados a você por outros stakeholders em sua organização.

O DataOps é um conjunto de ideias relativamente novo que ainda está em desenvolvimento, e nem todas as organizações adotaram as melhores práticas de DataOps. Em seu trabalho como engenheiro de dados, você pode se encontrar em uma organização onde o DataOps é bastante maduro ou em outra que ainda não abraçou o DataOps. A seguir, vamos dar uma olhada mais de perto na orquestração, que é um componente-chave do DataOps, e um componente tão crítico das arquiteturas e pipelines de dados modernos que consideramos como uma corrente subjacente separada do ciclo de vida da engenharia de dados.