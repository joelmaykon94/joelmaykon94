Claro! Aqui está a tradução:

---

Agora que você está familiarizado com o ciclo de vida da engenharia de dados e as correntes subjacentes, é hora de ver como esses conceitos se traduzem em ferramentas e tecnologias na nuvem AWS. Primeiro, só quero dizer que existem outros provedores de nuvem, incluindo o Azure da Microsoft e o Google Cloud Platform, além de outros provedores menores. E dependendo de onde você trabalha como engenheiro de dados, pode estar construindo em uma dessas outras plataformas. Dito isso, todos os conceitos que você está aprendendo nesses cursos serão relevantes para o seu trabalho, não importa em qual plataforma de nuvem você esteja construindo. Apenas alguns detalhes de ferramentas e implementação podem parecer um pouco diferentes.

Atualmente, a AWS é um provedor de nuvem líder, e estamos realmente animados por estar fazendo parceria com eles nesses cursos. Assim, você pode adquirir as habilidades técnicas necessárias para a engenharia de dados usando as mesmas ferramentas e tecnologias que foram adotadas por milhares de empresas ao redor do mundo. No próximo vídeo, Morgan Willis irá apresentar algumas das ferramentas com as quais você estará trabalhando nos laboratórios ao longo desses cursos, explicando onde essas ferramentas se encaixam no ciclo de vida da engenharia de dados e nas correntes subjacentes. Depois disso, eu lhe darei uma orientação sobre o exercício de laboratório para esta semana.
Claro! Aqui está a tradução:

---

Olá novamente. Espero que você tenha gostado da lição até agora sobre o ciclo de vida da engenharia de dados e as correntes subjacentes. Estou animado para compartilhar com você como esses conceitos se concretizam com ferramentas e tecnologias na nuvem AWS. Neste vídeo, vou passar por cada uma das etapas do ciclo de vida da engenharia de dados, assim como as correntes subjacentes que Joe apresentou até agora, e conectá-las a essas ferramentas específicas da nuvem que você usará para construir seus sistemas de dados.

Quando se trata de sistemas de origem na AWS, como Joe mencionou anteriormente, os sistemas de origem mais comuns com os quais você interagirá são os bancos de dados. Portanto, em vários laboratórios desses cursos, incluindo o desta semana, você estará trabalhando com o Amazon Relational Database Service, ou RDS. O RDS é um serviço que provisiona instâncias de banco de dados com o mecanismo de banco de dados relacional de sua escolha, como MySQL ou PostgreSQL. O RDS simplifica a sobrecarga operacional envolvida no provisionamento e hospedagem de um banco de dados relacional, além de cuidar de tarefas como correções e atualizações.

No próximo curso, você também trabalhará com o Amazon DynamoDB, que é uma opção de banco de dados NoSQL sem servidor. Com o DynamoDB, você cria tabelas independentes, que são virtualmente ilimitadas em seu tamanho total em todos os itens da tabela. O DynamoDB tem um esquema flexível e é mais adequado para aplicativos que requerem acesso de baixa latência a grandes volumes de dados, como jogos, IoT, aplicativos móveis e análises em tempo real, onde o modelo de dados pode evoluir ao longo do tempo sem a necessidade de migrações complexas.

Em termos de fontes de streaming, na última semana deste curso, você terá a oportunidade de trabalhar com o Amazon Kinesis Data Streams, que será configurado como um sistema de origem transmitindo atividades de usuários em tempo real de um blog de plataforma de vendas. Embora você não vá usar filas de mensagens como fontes nos laboratórios, pode se deparar com algo como o Amazon Simple Queue Service, ou SQS, para lidar com mensagens ao construir suas próprias pipelines de dados fora desses cursos. Outra opção popular para sistemas de origem de streaming é o Apache Kafka, que é uma plataforma de streaming de código aberto que você pode implementar por conta própria ou usar o serviço Amazon Managed Streaming for Kafka, ou MSK, que facilita a execução de cargas de trabalho Kafka na AWS, já que a infraestrutura subjacente é gerenciada para você. Você não usará o Kafka nos laboratórios, mas aprenderá os detalhes sobre o Kafka no curso dois.

Quando se trata de ingestão, se você estiver ingerindo de um banco de dados, pode usar o Amazon Database Migration Service, conhecido como DMS. Com o DMS, você pode migrar e replicar dados de uma fonte para um destino de forma automatizada. Mas para os laboratórios desses cursos, você usará principalmente o serviço AWS Glue ETL, que oferece recursos que suportam processos de integração de dados. E ao ingerir dados de uma fonte de streaming, você usará o Amazon Kinesis Data Streams e o Amazon Data Firehose nos laboratórios. Porém, no mundo real, você pode usar uma das outras ferramentas de ingestão de streaming que mencionei anteriormente, como SQS, Kafka ou outras.

Com o armazenamento em nuvem, você praticará usando opções tradicionais de armazém de dados, incluindo o Amazon Redshift, assim como armazenamento de objetos para um data lake no Amazon Simple Storage Service, conhecido como S3. Também veremos como você pode combinar serviços em um arranjo conhecido como lakehouse, para acesso contínuo tanto aos dados estruturados em seu armazém de dados quanto aos dados não estruturados em um data lake de armazenamento de objetos.

Na fase de transformação desses cursos, você trabalhará com AWS Glue, bem como Apache Spark e DBT, que são ferramentas que você pode usar em combinação com o Glue ou como alternativas, dependendo de suas necessidades. Quando se trata de servir dados, analisaremos os dois principais casos de uso potenciais, que são, primeiro, o caso de uso de inteligência de negócios ou análise, e segundo, um caso de uso de IA ou aprendizado de máquina. Para análises, você usará ferramentas como Amazon Athena ou Redshift para consultar dados estruturados e não estruturados. Você também terá alguma experiência trabalhando com um dashboard em um Jupyter Notebook no laboratório desta semana. E dependendo da empresa e da equipe com a qual você trabalha, pode também usar ferramentas de dashboard como Amazon QuickSight, assim como Apache Superset e Metabase, ambas opções de código aberto.

Para os casos de uso de IA e aprendizado de máquina, você servirá dados em lote para treinamento de modelos e trabalhará com algumas opções de bancos de dados vetoriais para servir dados para recomendadores de produtos e uso com grandes modelos de linguagem. É importante lembrar que para cada estágio do ciclo de vida da engenharia de dados, existem inúmeras outras opções de serviços gerenciados e de código aberto. Mas eu queria mencionar algumas específicas aqui para que você possa começar a conectar alguns dos conceitos que está aprendendo às ferramentas e tecnologias com as quais praticará nesses cursos.

No próximo vídeo, relacionarei cada uma das correntes subjacentes do ciclo de vida da engenharia de dados a conceitos e tecnologias na AWS. Vejo você lá.